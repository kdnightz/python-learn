# 解疑

1. ```python
   img.resize((width, height),Image.ANTIALIAS)
   第二个参数：
   Image.NEAREST ：低质量
   Image.BILINEAR：双线性
   Image.BICUBIC ：三次样条插值
   Image.ANTIALIAS：高质量
   ```

2. np.array()和np.asarray()

   1. array和asarray都可以将结构数据转化为ndarray，但是主要区别就是当数据源是ndarray时，array仍然会copy出一个副本，占用新的内存，但asarray不会。

3. ```python
   device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
   ```

4. LeNet

   * 两个layer（conv+pool）+ 两层网络 + 全连接层

   * ![22](img/22.png)

   ![23](img/23.png)

   

   * 过滤器f=5，即 每5×5个窗口滑动卷积
   * ![24](img/24.png)

5. 

   1. (w,y,w,h): xy是物体中心点位置，wh以及中心点距离物体两边的长宽
   2. xmin,ymin,xmax,ymax:物体位置的左上角（min），右下角（max）坐标。
   3. ![](/img/8.png)

6. 对于分类的概率，使用 交叉熵损失

7. 对于位置信息具体的数值，可使用 **MSE均方误差**损失（L2损失）

8. R-CNN

   1. 步骤
      1. 对一张图片，SS算法找出图片中可能存在目标的候选区域，2000个
      2. 2000个区域做大小变换(因为原框大小不固定)，输入到AlexNet中，得到 特征向量
         * 2000×4096维矩阵
      3. 经过20个类别的SVM分类器，对于2000个候选区域做判断，得到[2000,20]得分矩阵，即每个候选区域，都有每个类别做判断得分。
      4. 2000个候选区域，做NMS(non-maximum suppression:非极大值抑制),取出不好的，重叠度高的一些候选区域，得到剩下的 结果好的，分数高的框。
      5. 修正候选框，bbox回归微调

9. 候选区域(ROI)

10. 特征向量训练分类器SVM，见步骤3

   * R-CNN选用SVM二分类，假设20个类别，则得出[2000,20]的得分矩阵，每个向量都有20个类别的分数

11. 迭代过程，见步骤4

    * 对所选区域得分进行概率筛选 0.5   2000→5
    * 再，交并比计算，
      假如 真实物体为2(N),筛选后候选框为5(P)，计算N中每个物体位置和所有P的 IoU计算，得到P中每个候选框对应IoU最高的N中的一个。
    * ![](/img/9.png)

12. 修正候选区域，见步骤5

    * 为了让候选框标注更准确，去修正原来的位置
    * A是候选框，G是目标GT框，让A和G做回归训练，得到四个参数，

13. RCNN输出：一张图片预测一个X候选框， x × w = y_locate

    * y_locate 是真正算法输出的位置

14. IoU交并比

    1. 两个区域的重叠程度：= 重叠区域/联合区域、
    2. 0-1之间 越接近1 ，y_locate和ground_truth更接近

15. 平均精确率 map

    1. 定义：多个分类任务的AP的平均值
    2. (AP1+AP2+.....+AP20)/20
    3. 对于每个类别计算AP（AUC）的值：
       1. 对于猫类别：预测框预测是猫类别的概率做一个排序，得到一个预测框排序的列表（8个）
       2. 对于猫当中预测框排序列表（8个），进行计算AUC
       3. 最终得到20个类别，然后让20个AP相加，再求mAP
       4. 注意：精确率召回率：解决样本不均衡问题

16. 改进SPPnet

    1. 图片输入到网络先得到一个feature map
    2. 原图中通过SS得到的 **候选区域直接映射特征向量中对应位置**
       * 左上角的点：
         * x‘ =[x/S]+1
       * 右下角的点：
         * y' =[x/S]-1
       * 论文当中S = 2×2×2×2=16
       * 原图：特征图中
         * xmin', ymin' = [xmin/16]+1 , [ymin/16]-1
         * xmax', ymax' = [xmax/16]+1 , [ymax/16]-1
    3. 映射过来的(假如2000)候选区域的特征，经过SPP层（空间金字塔变换层），S输出固定大小的特征向量

17. **SPP 将 特征图转化成固定大小的特征向量**

    1. SPP layer 将每个候选区域分成 1×1,2×2,4×4三张子图，对每个子图的每个区域做max polling，得出的特征再连接到一起就是（16+4+1）×256=21×256=5376结果，接着全连接层做进一步处理，
    2. Spatial bins（空间盒个数）：1+4+16=21
    3. ![](/img/10.png)
    4. 一张图片直接卷积，再SS选择性搜索的候选框映射到特征图中，每个特征图池化（相当于21个盒子的池化），到FC层，FC层再经过SVM和微调。

18. Fast R-CNN

    1. 改进：提出了Rol pooling
    2. 分类使用softmax
    3. 与SPPNet一样的地方
       1. 首先将图片输入到一个基础卷积网络，得到整张图的feature map
       2. 将选择性搜索算法的结果region proposal（RoI）映射到feature map中
    4. RoI pooling
       1. 为了减少计算时间并得出固定长度的向量
       2. 使用一种4×4=16的空间盒子
       3. **所以比SPP快**
    5. 训练会比较统一：废弃了SVM和SPPNet
       1. 使用RoI pooling layer + softmax
    6. 多任务损失-Multi-task loss
       1. 平均绝对误差（MAE）损失即L1损失 + 交叉熵损失
    7. ![](/img/11.png)

19. Faster R-CNN

    1. 候选区域筛选融合到网络当中

       * 四个基本步骤（候选区域生成，特征提取，分类，位置精修）统一到 一个深度网络框架内

    2. 区域生成网络（RPN） + Fast R-CNN

    3. RPN 替代了 SS选择性算法

       ![](/img/12.png)

       1. RPN网络用于生成region proposals
       2. 通过softmax 判断anchors 属于 foreground 或者 background
       3. bounding box regression 修正 anchors获得精确的proposals
       4. 得到默认300个候选区域给RoI pooling
       5. 后面继续Fast R-CNN操作
       6. ![](/img/13.png)

    4. RPN原理

       1. 用 n×n （默认3×3=9）的大小窗口去扫描特征图得到K个候选窗口

          ![](/img/14.png)

       2. 每个特征图中像素对应9个窗口大小？

       3. 三种尺度{128,256,512}，三种长宽比{1:1,1:2,2:1}

       4. 3×3=9 不同大小的候选框

          1. 窗口输出[N,256] --> 分类：判断是否是背景
          2. 回归位置：N个候选框与自己对应的目标值GT做回归，修正位置。
          3. 这里是得到更好的候选区域，提供给RoI pooling使用

    5. Faster R-CNN 训练

       1. RPN训练

          * 分类：二分类，softmax，logisticregression

          * 候选框的调整：均方误差做修正

       2. Fast R-CNN 部分的训练

          * 预测类别训练 softmax
          * 和预测位置的训练：均方误差损失

       3. 样本准备：正负anchors样本比例  1:3

          ![](/img/15.png)

       4. 





1. x = c if a else b

   1. ```python
      #当 a = True , x =c 
      #当 a = False , x = b
      if a :
      	x = c
      else:
      	x = b
      ```

2. nn.Conv2d

   * 二维卷积可以处理二维数据
     nn.Conv2d(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True))
     * **参数：**
       * in_channel:　输入数据的通道数，例RGB图片通道数为3；
       * out_channel: 输出数据的通道数，这个根据模型调整；
       * kennel_size: 卷积核大小，可以是int，或tuple；kennel_size=2,意味着卷积大小(2,2)， kennel_size=（2,3），意味着卷积大小（2，3）即非正方形卷积
       * stride：步长，默认为1，与kennel_size类似，stride=2,意味着步长上下左右扫描皆为2， stride=（2,3），左右扫描步长为2，上下为3；
       * padding：　零填充



3. YOLO

   1. GoogleNet + 4个卷积 + 2个全连接层

   2. 网络输出大小：7×7×30：30层，每层切分为7×7=49个像素

   3. 流程

      1. 单元格：

         * 7×7=49个像素值，49个单元格

         1. 每个单元格负责预测一个物体类别，并直接预测物体概率值
         2. 每个单元格 预测 两个（默认）bbox位置，两个bbox置信度(confidence )
            * 一个bbox：xmin,ymin,xmax,ymax,confidence 
            * 两个bbox：4 + 1 + 4 + 1 = 10个值
            * 30：10个给bbox，20个给20类的预测概率结果

      2. 网格输出筛选

         1. ![](/img/16.png)
         2. 一个网格会预测两个Bbox，但是在训练时我们只有一个Bbox专门负责预测概率（一个Object，一个Bbox）
         3. 那20个类别概率代表这个网格中一个bbox
         4. 每个bbox都有对应给个confidence score
            1. 如果**grid cell（网格）**里面没有object，则confidence为0
            2. 如果有，则confidence score 等于 预测的box 和 ground truth （GT）的IOU乘积
               1. 判断cell里有object：一个object的ground truth的中心点在网格里
               2. 两个bbox的四个值都与GT进行IoU计算，得到两个IoU值。
         5. ![](/img/17.png)

   4. 不用于Faster R-CNN中的 anchors，yolo的框，概率值都是由网络输出，7×7×30（人为给30赋了具体的定义）

   5. 训练：

      1. 预测框对应的目标值标记，如上图。

      2. 理解：

         ![](/img/18.png)

         1. 损失：
            * 三部分损失：bbox损失 + confidence损失 + classficattion损失



1. 卷积神经网络

   * 包含：

     1. 卷积层（convolution）

        * 卷积运算过程：需要卷积核
          * 大小：奇数大小的过滤器，1×1,3×3,5×5
        * 1、正常卷积核运算，默认移动一个像素
          * 缺点 ：图片变小，边缘信息丢失
        * 2、零填充
          * 因为0在权重乘积和运算中对最终结果不造成影响，也就避免了图片增加了额外的干扰信息。
          * ![19](/img/19.png)
          * 两种方式
            * Valid：不填充，结果变小
            * SAME：输出图像与原图相等大小
          * 注意：由于避免零填充不均匀，所以我们需要奇数大小的过滤器,F即卷积核为奇数
        * 3、步长
          * 如果步长不为默认1的情况下，那么
            * 图片大小N,过滤器大小F,步长S,零填充P
            * 卷积后尺寸：((N+2P-F)/S+1),((N+2P-F)/S+1) (原尺寸 + 2*零填充-卷积核) /步长
        * 4、多通道的图片：Filter必须使用相同的通道数
        * 5、多个卷积核数量
          * 得到的特征图大小没有影响，而数量会变化，多少卷积核多少特征图

     2. 卷积层的运算结果

        * 假设神经网络某层  ζ 的输入：

        ![notknow1](img/notknow1.png)

     3. 池化层（subsampling）

        * 最大池化
          * ![20](img/20.png)
        * 平均池化
        * 窗口设置
          * 2×2 ，两个步长，没有参数
        * 作用
          * 降低了后续网络层的输入维度，缩减模型大小，提高计算速度
          * 提高了Feature Map的鲁棒性，防止过拟合

     4. 全连接层（full connection）

        * ![21](img/21.png)

     5. 激活函数
   
2. 经典分类卷积网络结构

