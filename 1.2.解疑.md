# 解疑

1. ```python
   img.resize((width, height),Image.ANTIALIAS)
   第二个参数：
   Image.NEAREST ：低质量
   Image.BILINEAR：双线性
   Image.BICUBIC ：三次样条插值
   Image.ANTIALIAS：高质量
   ```

2. np.array()和np.asarray()

   1. array和asarray都可以将结构数据转化为ndarray，但是主要区别就是当数据源是ndarray时，array仍然会copy出一个副本，占用新的内存，但asarray不会。

3. ```python
   device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
   ```

4. 

   1. (w,y,w,h): xy是物体中心点位置，以及中心点距离物体两边的长宽
   2. xmin,ymin,xmax,ymax:物体位置的左上角（min），右下角（max）坐标。
   3. ![](/img/8.png)

5. 对于分类的概率，使用 交叉熵损失

6. 对于位置信息具体的数值，可使用 **MSE均方误差**损失（L2损失）

7. R-CNN

   1. 步骤
      1. 对一张图片，SS算法找出图片中可能存在目标的候选区域，2000个
      2. 2000个区域做大小变换(因为原框大小不固定)，输入到AlexNet中，得到 特征向量
         * 2000×4096维矩阵
      3. 经过20个类别的SVM分类器，对于2000个候选区域做判断，得到[2000,20]得分矩阵，即每个候选区域，都有每个类别做判断得分。
      4. 2000个候选区域，做NMS(non-maximum suppression:非极大值抑制),取出不好的，重叠度高的一些候选区域，得到剩下的 结果好的，分数高的框。
      5. 修正候选框，bbox回归微调

8. 候选区域(ROI)

9. 特征向量训练分类器SVM，见步骤3

   * R-CNN选用SVM二分类，假设20个类别，则得出[2000,20]的得分矩阵，每个向量都有20个类别的分数

10. 迭代过程，见步骤4

    * 对所选区域得分进行概率筛选 0.5   2000→5
    * 再，交并比计算，
      假如 真实物体为2(N),筛选后候选框为5(P)，计算N中每个物体位置和所有P的 IoU计算，得到P中每个候选框对应IoU最高的N中的一个。
    * ![](/img/9.png)

11. 修正候选区域，见步骤5

    * 为了让候选框标注更准确，去修正原来的位置
    * A是候选框，G是目标GT框，让A和G做回归训练，得到四个参数，

12. RCNN输出：一张图片预测一个X候选框， x × w = y_locate

    * y_locate 是真正算法输出的位置

13. IoU交并比

    1. 两个区域的重叠程度：= 重叠区域/联合区域、
    2. 0-1之间 越接近1 ，y_locate和ground_truth更接近

14. 平均精确率 map

    1. 定义：多个分类任务的AP的平均值
    2. (AP1+AP2+.....+AP20)/20
    3. 对于每个类别计算AP（AUC）的值：
       1. 对于猫类别：预测框预测是猫类别的概率做一个排序，得到一个预测框排序的列表（8个）
       2. 对于猫当中预测框排序列表（8个），进行计算AUC
       3. 最终得到20个类别，然后让20个AP相加，再求mAP
       4. 注意：精确率召回率：解决样本不均衡问题

15. 改进SPPnet

    1. 图片输入到网络先得到一个feature map
    2. 原图中通过SS得到的 **候选区域直接映射特征向量中对应位置**
       * 左上角的点：
         * x‘ =[x/S]+1
       * 右下角的点：
         * y' =[x/S]-1
       * 论文当中S = 2×2×2×2=16
       * 原图：特征图中
         * xmin', ymin' = [xmin/16]+1 , [ymin/16]-1
         * xmax', ymax' = [xmax/16]+1 , [ymax/16]-1
    3. 映射过来的(假如2000)候选区域的特征，经过SPP层（空间金字塔变换层），S输出固定大小的特征向量

16. **SPP 将 特征图转化成固定大小的特征向量**

    1. SPP layer 将每个候选区域分成 1×1,2×2,4×4三张子图，对每个子图的每个区域做max polling，得出的特征再连接到一起就是（16+4+1）×256=21×256=5376结果，接着全连接层做进一步处理，
    2. Spatial bins（空间盒个数）：1+4+16=21
    3. ![](/img/10.png)
    4. 一张图片直接卷积，再SS选择性搜索的候选框映射到特征图中，每个特征图池化（相当于21个盒子的池化），到FC层，FC层再经过SVM和微调。
    
17. Fast R-CNN

    1. 改进：提出了Rol pooling
    2. 分类使用softmax
    3. 与SPPNet一样的地方
       1. 首先将图片输入到一个基础卷积网络，得到整张图的feature map
       2. 将选择性搜索算法的结果region proposal（RoI）映射到feature map中
    4. RoI pooling
       1. 为了减少计算时间并得出固定长度的向量
       2. 使用一种4×4=16的空间盒子
       3. **所以比SPP快**
    5. 训练会比较统一：废弃了SVM和SPPNet
       1. 使用RoI pooling layer + softmax
    6. 多任务损失-Multi-task loss
       1. 平均绝对误差（MAE）损失即L1损失 + 交叉熵损失
    7. ![](/img/11.png)

18. Faster R-CNN

    1. 候选区域筛选融合到网络当中

       * 四个基本步骤（候选区域生成，特征提取，分类，位置精修）统一到 一个深度网络框架内

    2. 区域生成网络（RPN） + Fast R-CNN

    3. RPN 替代了 SS选择性算法

       ![](/img/12.png)

       1. RPN网络用于生成region proposals
       2. 通过softmax 判断anchors 属于 foreground 或者 background
       3. bounding box regression 修正 anchors获得精确的proposals
       4. 得到默认300个候选区域给RoI pooling
       5. 后面继续Fast R-CNN操作
       6. ![](/img/13.png)

    4. RPN原理

       1. 用 n×n （默认3×3=9）的大小窗口去扫描特征图得到K个候选窗口

          ![](/img/14.png)

       2. 每个特征图中像素对应9个窗口大小？

       3. 三种尺度{128,256,512}，三种长宽比{1:1,1:2,2:1}

       4. 3×3=9 不同大小的候选框

          1. 窗口输出[N,256] --> 分类：判断是否是背景
          2. 回归位置：N个候选框与自己对应的目标值GT做回归，修正位置。
          3. 这里是得到更好的候选区域，提供给RoI pooling使用

    5. Faster R-CNN 训练

       1. RPN训练

          * 分类：二分类，softmax，logisticregression

          * 候选框的调整：均方误差做修正

       2. Fast R-CNN 部分的训练

          * 预测类别训练 softmax
          * 和预测位置的训练：均方误差损失

       3. 样本准备：正负anchors样本比例  1:3

          ![](/img/15.png)

       4. 